{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bb75431",
   "metadata": {},
   "source": [
    "here we should do all pip install's in terminal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739afae8",
   "metadata": {},
   "source": [
    "error 1: now streamlit app is running locally, if ingested pfd and its showing like this \n",
    "                                 \n",
    "Both `max_new_tokens` (=256) and `max_length`(=220) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n",
    "\n",
    "This is not an error, but a warning from hugging face transformer library,\n",
    "Why it happens?\n",
    "When generating text, the model looks at parameters like:\n",
    "max_length → total length of input + output tokens\n",
    "max_new_tokens → maximum number of tokens the model can generate in the output\n",
    "\n",
    "If you set both, the library says:\n",
    "“I’ll ignore max_length and use max_new_tokens instead.”\n",
    "That’s why you see the message.\n",
    "\n",
    "How to fix (optional)\n",
    "You can update your synthesize_answer function (or wherever you call .generate()) to only set one of them. For example:\n",
    "\n",
    "output = generator(\n",
    "    query,\n",
    "    max_new_tokens=256,   # keep only this\n",
    "    # remove max_length to avoid the warning\n",
    "    do_sample=True,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "\n",
    "If you’re fine with the current behavior, you can safely ignore the warning — it won’t break your chatbot.\n",
    "But if you want a clean console, just remove max_length and use only max_new_tokens.\n",
    "\n",
    "You should use only one:\n",
    "If you want the model to generate at most N tokens beyond the prompt, use:\n",
    "generator(prompt, max_new_tokens=256)\n",
    "\n",
    "If you want the model to limit total tokens (prompt + answer), use:\n",
    "generator(prompt, max_length=220)\n",
    "\n",
    "In practice, max_new_tokens is recommended for QA/chat apps, because your prompt length can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831e712d",
   "metadata": {},
   "source": [
    "Error 2: about chunks & MemoryError\n",
    "\n",
    "File \"C:\\Users\\Admin\\Documents\\CusorAIDoc\\chatbot_st.py\", line 47, in <module> chunks = chunk_text(paste_text,chunk_size=chunk_size,overlap=overlap) File \"C:\\Users\\Admin\\Documents\\CusorAIDoc\\csb.py\", line 44, in chunk_text chunks.append(chunk) ~~~~~~~~~~~~~^^^^^^^\n",
    "\n",
    "First thought it about appending chunks without initial like chunks = [], but the problem was that my prgm was ruuning out of RAM,while spiltting and storing the chunks.\n",
    "\n",
    "Why is this happening?\n",
    "Looking at your code:\n",
    "\n",
    "while start < n:\n",
    "    end = min(start + chunk_size, n)\n",
    "    chunk = text[start:end]\n",
    "    chunks.append(chunk)\n",
    "    start = end - overlap\n",
    "    if start < 0:\n",
    "        start = 0\n",
    "\n",
    "\n",
    "If overlap is too big (e.g. overlap=120 with chunk_size=800), the loop may get stuck creating too many overlapping chunks.\n",
    "If the input text (paste_text) is very large (say an entire big PDF), storing every chunk in a list eats up RAM.\n",
    "If start is not increasing properly, it can loop infinitely → filling memory.\n",
    "\n",
    "How to Fix:\n",
    "Fix the loop increment.Right now, you are doing:\n",
    "start = end - overlap\n",
    "\n",
    "This can cause overlaps to pile up incorrectly. Instead, you should move forward like this:\n",
    "start += chunk_size - overlap\n",
    "\n",
    "Add a safeguard for memory\n",
    "If the text is too big, limit chunks:\n",
    "if len(chunks) > 10000:   # arbitrary cutoff\n",
    "    break\n",
    "\n",
    "Optimized function:\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 800, overlap: int = 120) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks.\n",
    "    \"\"\"\n",
    "    text = clean_text(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    n = len(text)\n",
    "\n",
    "    while start < n:\n",
    "        end = min(start + chunk_size, n)\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # move start forward correctly\n",
    "        start += chunk_size - overlap  \n",
    "\n",
    "        if start < 0:  \n",
    "            start = 0  \n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7ffd3e",
   "metadata": {},
   "source": [
    "error 3:after running streamlit run locally, i stored the name in data/faqs.txt and asked chat, it saying no source found.\n",
    "\n",
    "The reason it says “No sources found yet. Insert your documents.” is because your app didn’t actually load the data/faqs.txt file into the vector database / memory for retrieval.\n",
    "\n",
    "But saving text manually into data/faqs.txt does not automatically load it into the app. The app only works with documents you upload or paste + press Insert.\n",
    "\n",
    "How to Fix\n",
    "You have two options:\n",
    "\n",
    "Option 1: Upload your file via UI\n",
    "Go to the left panel in your app.\n",
    "Click Browse files → select your faqs.txt.\n",
    "Press Insert.\n",
    "→ This will chunk the file and add it to your knowledge base.\n",
    "Now when you ask “what is my name?”, it should search inside faqs.txt.\n",
    "\n",
    "Option 2: Auto-load data/faqs.txt on startup\n",
    "If you want your app to always load data/faqs.txt without uploading each time, add this code in your chatbot_st.py (before the UI part):\n",
    "\n",
    "import os\n",
    "\n",
    "faqs_path = \"data/faqs.txt\"\n",
    "if os.path.exists(faqs_path):\n",
    "    with open(faqs_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        faq_text = f.read()\n",
    "    chunks = chunk_text(faq_text, chunk_size=800, overlap=120)\n",
    "    # insert into your vector store\n",
    "    insert_chunks(chunks)\n",
    "\n",
    "\n",
    "(Here insert_chunks should be whatever function you are already using when pressing Insert in your app.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6315cf6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
