{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "id": "YD6lJRno16M1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncYNwM19zEZL"
      },
      "outputs": [],
      "source": [
        "import os   #let's us to interact with computer OS. eg, create/delete folders,read files,check file paths etc...\n",
        "import io   #stands for i/p o/p .Let's us work with data as if it's a file(even if it's in a memory)\n",
        "import re   #Regular expression. helps to find or replace pattern in text\n",
        "import json  #used to work with JSON data(common format to store data like a dict)\n",
        "import pandas as pd  #pandas-->data handling module, popular library for data analysis,works with table(rows and column) like excel\n",
        "import numpy as np  #numpy-->data handling module,library for numerical calculation,great for working with arrays,matrics and math operation\n",
        "import time  #let's us work with time(pause pgm,measure how long something takes)\n",
        "import streamlit as st  #stramlit-->app & dashboard,used to make web apps for DS & ML\n",
        "from typing import List,Dict,Tuple,Any  #type hinting(not code execution,just for clarity),helps to explain datatypes in function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e.g for type hinting\n",
        "\n",
        "def add_numbers(numbers: List[int]) -> int:\n",
        "   return sum(numbers)\n",
        "\n",
        "list[int] means the input is a list of integer\n",
        "-> int means the function returns as integer"
      ],
      "metadata": {
        "id": "5mkkaz4D7TMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import faiss\n",
        "from pypdf import PdfReader\n",
        "from docx import Document as DocxDocument"
      ],
      "metadata": {
        "id": "YF5tUjO011hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP & AI models\n",
        "1.from sentence_transformers import SentenceTransformer\n",
        "   library to create sentence embbedding(turns sentence into num vector);useful for sementaic search,recommandation,clustering\n",
        "   Example:\n",
        "   \"I love cats\" → [0.12, -0.55, 0.88, ...] (vector).\n",
        "\n",
        "2.from transformers import pipeline\n",
        "   Hugging Face Transformers library.\n",
        "   pipeline is a shortcut to use pre-trained AI models easily.\n",
        "   Example tasks: text summarization, translation, sentiment analysis, question answering.\n",
        "   Example:\n",
        "   from transformers import pipeline\n",
        "   summarizer = pipeline(\"summarization\")\n",
        "   print(summarizer(\"I love tom\"))\n",
        "\n",
        "Searching Similar Data\n",
        "1.import faiss\n",
        "  FAISS (by Facebook/Meta) is for fast similarity search.\n",
        "  Helps you search quickly through millions of vectors (like sentence embeddings).\n",
        "  Example: find the most similar sentence/document to a query.\n",
        "\n",
        "Reading Files\n",
        "1.from pypdf import PdfReader\n",
        "  Library to read text from PDF files.\n",
        "  Example: load a PDF, get number of pages, extract text.\n",
        "\n",
        "from docx import Document as DocxDocument\n",
        "Library to read and write Word documents (.docx).\n",
        "Example: open a .docx file and read paragraphs, or create a new Word file.\n",
        "\n",
        "\n",
        "In short:\n",
        "\n",
        "SentenceTransformer → turns sentences into numbers (vectors).\n",
        "pipeline → quick way to use AI models (summarize, translate, classify).\n",
        "faiss → finds similar vectors really fast (used for search).\n",
        "PdfReader → extracts text from PDFs.\n",
        "DocxDocument → reads/writes Word docs."
      ],
      "metadata": {
        "id": "Frn8O3wn-XoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "id": "W8jzc2ap8pyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "id": "5lrWIhnB8wm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "id": "vACXofJm9RaQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#clean & chunk text\n",
        "def clean_text(text:str)->str:\n",
        "  text=re.sub(r\"\\s+\",\" \",text)\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "lo_99tpw9gcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it does step by step:\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "Defines a function called clean_text.\n",
        "It takes some text as input (text).\n",
        "-> str means it will return a string.\n",
        "\n",
        "re.sub(r\"\\s+\", \" \", text)\n",
        "Uses regular expressions (re).\n",
        "\\s+ means \"one or more spaces, tabs, or newlines\".\n",
        "Replaces them with a single space \" \".\n",
        "Example: \"Hello World\\n\\nHow are you?\" → \"Hello World How are you?\".\n",
        "\n",
        "text.strip()\n",
        "Removes extra spaces at the start and end of the text.\n",
        "Example: \" Hello World \" → \"Hello World\"."
      ],
      "metadata": {
        "id": "wyWYfJp_l3IY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text:str, chunk_size:int=800, overlap:int=120) -> List[str]:\n",
        "  \"\"\"\n",
        "  Character based chunking(simple & Robust)\n",
        "  Chunk size ~800 chars works well for small model like FLAN-T5\n",
        "  \"\"\"\n",
        "  text = clean_text(text)\n",
        "  chunks = []\n",
        "  start = 0\n",
        "  n = len(text)\n",
        "  while start < n:\n",
        "    end = min(start + chunk_size, n)\n",
        "    chunk = text[start:end]\n",
        "    chunks.append(chunk)\n",
        "    start=end-overlap\n",
        "    if start < 0:\n",
        "      start = 0\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "1CFUGGn6FZ6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it does\n",
        "\n",
        "This function splits long text into smaller pieces (chunks).\n",
        "Inputs\n",
        "text: str → the text you want to split.\n",
        "chunk_size: int = 800 → how many characters per chunk (default 800).\n",
        "overlap: int = 120 → how many characters overlap between chunks (default 120).\n",
        "Returns: a list of text chunks (List[str]).\n",
        "\n",
        "text = clean_text(text)\n",
        "Cleans the text (removes extra spaces/newlines).\n",
        "\n",
        "Initialize variables\n",
        "chunks = [] → empty list to store the text pieces.\n",
        "start = 0 → where we start cutting.\n",
        "n = len(text) → total number of characters in text.\n",
        "\n",
        "Loop until we reach the end of the text\n",
        "end = min(start + chunk_size, n)\n",
        "→ defines where the chunk ends.\n",
        "chunk = text[start:end]\n",
        "→ takes a slice of the text.\n",
        "chunks.append(chunk)\n",
        "→ saves the chunk in the list.\n",
        "\n",
        "Overlap handling\n",
        "start = end - overlap\n",
        "→ moves the start backward by 120 chars so the next chunk overlaps with the previous one.\n",
        "This is important because AI models may lose context if chunks don’t overlap.\n",
        "\n",
        "Return chunks\n",
        "After looping, we return the list of all chunks."
      ],
      "metadata": {
        "id": "GYdjG-3voEUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#file loader(PDF,DOCX,CSV,TXT)"
      ],
      "metadata": {
        "id": "je2IrqM7HdE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_txt(file_bytes: bytes)->str:\n",
        "  return file_bytes.decode(\"utf-8\",errors=\"ignore\")"
      ],
      "metadata": {
        "id": "67VQ7smJI38p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it does\n",
        "\n",
        "file_bytes: bytes\n",
        "The function takes a file that’s been read in bytes form (raw computer data).\n",
        "Example: when you upload a .txt file, it is often read as bytes first.\n",
        "\n",
        ".decode(\"utf-8\", errors=\"ignore\")\n",
        "Converts those bytes into a human-readable string using the utf-8 text format (the most common text encoding).\n",
        "errors=\"ignore\" means: if there are weird symbols it can’t decode, just skip them instead of crashing.\n",
        "\n",
        "Returns\n",
        "A string version of the file’s contents."
      ],
      "metadata": {
        "id": "EEeUjr9lo7Xe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(file_bytes: bytes)->str:\n",
        "  with io.BytesIO(file_bytes) as fb:\n",
        "    reader = PdfReader(fb)\n",
        "    texts=[]\n",
        "    for page in reader.pages:\n",
        "      try:\n",
        "        t=page.extract_text() or \"\"\n",
        "      except Exception :\n",
        "        t=\"\"\n",
        "      if t:\n",
        "        texts.append(t)\n",
        "  return \"\\n\".join(texts)"
      ],
      "metadata": {
        "id": "QlEAm7w8JUxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it does\n",
        "\n",
        "This function takes a PDF file in bytes and extracts all the text inside it.\n",
        "Step by step:\n",
        "with io.BytesIO(file_bytes) as fb:\n",
        "Turns the raw file bytes into a file-like object (fb) so that PdfReader can read it (like opening a real file).\n",
        "\n",
        "reader = PdfReader(fb)\n",
        "Creates a PdfReader object to work with the PDF.\n",
        "\n",
        "texts = []\n",
        "An empty list to collect text from each page.\n",
        "\n",
        "Loop through each page\n",
        "for page in reader.pages:\n",
        "Goes page by page inside the PDF.\n",
        "\n",
        "Extract text safely\n",
        "try:\n",
        "    t = page.extract_text() or \"\"\n",
        "except Exception:\n",
        "    t = \"\"\n",
        "page.extract_text() → tries to pull text from the page.\n",
        "or \"\" → if it returns None, replace with an empty string.\n",
        "except → if extraction fails (like scanned PDFs with images), it just skips.\n",
        "\n",
        "Add text if available\n",
        "if t:\n",
        "    texts.append(t)\n",
        "Saves the text from that page into the list.\n",
        "\n",
        "Return joined text\n",
        "return \"\\n\".join(texts)\n",
        "Combines all page texts into one big string, separated by newlines."
      ],
      "metadata": {
        "id": "MX78zFknqZz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_docx(file_bytes: bytes)->str:\n",
        "  with io.BytesIO(file_bytes) as fb:\n",
        "    doc=DocxDocument(fb)\n",
        "  return \"\\n\".join(p.text for p in doc.paragraphs)"
      ],
      "metadata": {
        "id": "0UPYU4Q0UnkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a Word file (.docx) in bytes and extracts all the text inside it.\n",
        "\n",
        "Step by step:\n",
        "with io.BytesIO(file_bytes) as fb:\n",
        "Turns the raw file bytes into a file-like object (fb), so Python can treat it like an actual .docx file.\n",
        "\n",
        "doc = DocxDocument(fb)\n",
        "Opens the Word document using the python-docx library.\n",
        "\n",
        "doc.paragraphs\n",
        "Gives you a list of all the paragraphs in the Word document.\n",
        "Each p is a paragraph object, and p.text is its text content.\n",
        "\n",
        "\"/n\".join(p.text for p in doc.paragraphs)\n",
        "Goes through each paragraph (p.text) and joins them together into one big string."
      ],
      "metadata": {
        "id": "a2YNMT5TtJ9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv(file_bytes: bytes)->str:\n",
        "  with io.BytesIO(file_bytes) as fb:\n",
        "    df=pd.read_csv(fb)\n",
        "    #converting to readable FAQ-like table text\n",
        "  return df.to_csv(index=False)"
      ],
      "metadata": {
        "id": "S_dm4tTEZgxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a CSV file in bytes and converts it into a text format (CSV string).\n",
        "\n",
        "Step by step:\n",
        "with io.BytesIO(file_bytes) as fb:\n",
        "Converts the raw bytes into a file-like object (fb) so pandas can read it.\n",
        "\n",
        "df = pd.read_csv(fb)\n",
        "Reads the CSV file into a pandas DataFrame (like an Excel table in Python).\n",
        "\n",
        "Comment: # converting to readable FAQ-like table text\n",
        "This means the idea is to convert the data into a nice text format (like Q&A style), but in this code they just output it back as CSV text.\n",
        "\n",
        "return df.to_csv(index=False)\n",
        "Converts the DataFrame back into a CSV string, without the row index.\n",
        "Basically, it gives you the CSV contents as plain text."
      ],
      "metadata": {
        "id": "puMi-Cdut_qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_any(file)->Tuple[str,str]:\n",
        "  name=file.name.lower()\n",
        "  content=file.read()\n",
        "  if name.endswith(\".pdf\"):\n",
        "    return \"pdf\",load_pdf(content)\n",
        "  elif name.endswith(\".docx\"):\n",
        "    return \"docx\",load_docx(content)\n",
        "  elif name.endswith(\".csv\"):\n",
        "    return \"csv\",load_csv(content)\n",
        "  elif name.endswith(\".txt\"):\n",
        "    return \"txt\",load_txt(content)\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported file type.Please upload PDF,DOCX,TXT, OR CSV.\")"
      ],
      "metadata": {
        "id": "bS6DobL1aXrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function can read different types of files (PDF, DOCX, CSV, TXT) using the right loader function.\n",
        "\n",
        "Step by step:\n",
        "name = file.name.lower()\n",
        "Gets the file name (like \"example.PDF\").\n",
        "Converts it to lowercase so it’s easier to check extensions (e.g., \"pdf\" vs \"PDF\").\n",
        "\n",
        "content = file.read()\n",
        "Reads the file content into memory (as bytes).\n",
        "\n",
        "Check the file type using extension\n",
        "If the file ends with .pdf → call load_pdf(content)\n",
        "If .docx → call load_docx(content)\n",
        "If .csv → call load_csv(content)\n",
        "If .txt → call load_txt(content)\n",
        "\n",
        "Return a tuple\n",
        "First item = file type (like \"pdf\", \"docx\", etc.).\n",
        "Second item = the extracted text from the file.\n",
        "\n",
        "Unsupported file\n",
        "If the file type isn’t recognized, it raises an error telling the user only PDF, DOCX, TXT, or CSV are allowed."
      ],
      "metadata": {
        "id": "cll5Vdahu9qF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding + FAISS handling\n",
        "@st.cache_resource\n",
        "def get_embedder():\n",
        "  return SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "pYiSxnPed-VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What it does\n",
        "\n",
        "@st.cache_resource\n",
        "This is a Streamlit decorator.\n",
        "It tells Streamlit:\n",
        "“Run this function only once and cache (remember) the result.”\n",
        "So if you call get_embedder() many times in the app, it won’t reload the model every time (saves time).\n",
        "\n",
        "def get_embedder():\n",
        "Defines a function named get_embedder.\n",
        "\n",
        "SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "Loads a pretrained embedding model from sentence-transformers.\n",
        "Model name: all-MiniLM-L6-v2 (a small, fast, but good model for embeddings).\n",
        "This model converts sentences into embeddings (numeric vectors).\n",
        "Example: \"Hello world\" → [0.23, -0.51, 0.88, ...]\n",
        "\n",
        "return SentenceTransformer(...)\n",
        "The function returns the model object.\n",
        "So now, whenever you need embeddings, you just call:\n",
        "embedder = get_embeder()\n",
        "vectors=embedder.encode([\"hi sam\",\"how u doing?\"])\n",
        "\n",
        "summary:\n",
        "This function loads a text embedding model once, caches it, and reuses it.\n",
        "The model converts sentences → vectors (embeddings).\n",
        "These embeddings are later used with FAISS for similarity search."
      ],
      "metadata": {
        "id": "cKsyDRIww7nD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_or_load_index(\n",
        "    embedder:SentenceTransformer,\n",
        "    storage_dir:str=\"storage\"\n",
        ") ->Tuple[faiss.IndexFlatL2,List[dict[str,Any]]]:\n",
        "    os.makedirs(storage_dir,exist_ok=True)\n",
        "    index_path=os.path.join(storage_dir,\"faiss.index\")\n",
        "    meta_path=os.path.join(storage_dir,\"meta.npy\")\n",
        "\n",
        "    if os.path.exists(index_path) and os.path.exists(meta_path):\n",
        "        index=faiss.read_index(index_path)\n",
        "        metadata=np.load(meta_path,allow_pickle=True).tolist()\n",
        "    return index,metadata\n",
        "\n",
        "    #Empty new index\n",
        "    index=faiss.IndexFlatL2(384)\n",
        "    metadata: List[Dict[str,Any]]=[]\n",
        "    return index,metadata\n"
      ],
      "metadata": {
        "id": "Krd2u4uvels-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function either loads an existing FAISS index (and metadata) from disk or, if none exists, creates a new empty index.\n",
        "\n",
        "Step by step:\n",
        "Function definition\n",
        "Takes in an embedder (the SentenceTransformer model).\n",
        "storage_dir=\"storage\" → default folder where index files are saved.\n",
        "Returns:\n",
        "a FAISS index (faiss.IndexFlatL2)\n",
        "metadata (list of dictionaries with extra info about each chunk).\n",
        "\n",
        "Paths for storage\n",
        "index_path = os.path.join(storage_dir, \"faiss.index\")\n",
        "meta_path  = os.path.join(storage_dir, \"meta.npy\")\n",
        "\n",
        "One file for the FAISS index.\n",
        "One file for the metadata.\n",
        "\n",
        "Check if files exist\n",
        "if os.path.exists(index_path) and os.path.exists(meta_path):\n",
        "    index = faiss.read_index(index_path)        # load FAISS index\n",
        "    metadata = np.load(meta_path, allow_pickle=True).tolist()  # load metadata\n",
        "    return index, metadata\n",
        "If both files are present → load them from disk and return.\n",
        "\n",
        "If no files found → create a new index\n",
        "index = faiss.IndexFlatL2(384)\n",
        "metadata: List[Dict[str, Any]] = []\n",
        "return index, metadata\n",
        "Creates a new FAISS index with vectors of size 384 (because \"all-MiniLM-L6-v2\" embeddings are 384-dimensional).\n",
        "Starts with an empty metadata list.\n",
        "\n",
        "Example usage\n",
        "embedder = get_embedder()\n",
        "index, metadata = build_or_load_index(embedder)\n",
        "\n",
        "print(index.ntotal)   # number of vectors stored\n",
        "print(len(metadata))  # number of metadata entries\n",
        "\n",
        "\n",
        "If nothing exists yet → you get an empty FAISS index.\n",
        "If files exist → it resumes from saved data.\n",
        "\n",
        "In simple terms:\n",
        "This function makes sure you always have a FAISS index ready:\n",
        "If you’ve already built and saved one → it loads it.\n",
        "If not → it creates a new, empty one.\n",
        "\n",
        "What is a FAISS index?\n",
        "\n",
        "FAISS = Facebook AI Similarity Search.\n",
        "A FAISS index is like a special database designed to store vectors (embeddings) and let you quickly find the ones that are most similar to a query.\n",
        "\n",
        "Think of it as:\n",
        "You turn sentences into number vectors using SentenceTransformer.\n",
        "You put those vectors inside a FAISS index.\n",
        "Later, when you give a new query (also turned into a vector), FAISS searches the index and finds the closest vectors (i.e., most similar sentences).\n",
        "So instead of searching text directly, you’re searching based on semantic meaning.\n",
        "\n",
        "What is faiss.IndexFlatL2?\n",
        "\n",
        "FAISS provides different types of indexes.\n",
        "IndexFlatL2 means:\n",
        "Flat → It stores all vectors directly (no fancy compression, no approximation).\n",
        "L2 → It measures similarity using L2 distance (Euclidean distance).\n",
        "That’s just the usual \"straight-line distance\" in multi-dimensional space.\n",
        "\n",
        "sumaary:\n",
        "FAISS index = a smart database for embeddings.\n",
        "IndexFlatL2 = the simplest type of FAISS index, which stores vectors and compares them using Euclidean distance to find similar ones.\n",
        "\n",
        "Later, you can use more advanced FAISS indexes (IndexIVF, HNSW, etc.) for faster searches when you have millions of vectors."
      ],
      "metadata": {
        "id": "RSCmZ0kOzq-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def persist_index(index: faiss.IndexFlatL2, metadata: List[Dict[str, Any]], storage_dir: str = \"storage\"):\n",
        "    os.makedirs(storage_dir, exist_ok=True)\n",
        "    faiss.write_index(index, os.path.join(storage_dir, \"faiss.index\"))\n",
        "    np.save(os.path.join(storage_dir, \"meta.npy\"), np.array(metadata, dtype=object), allow_pickle=True)"
      ],
      "metadata": {
        "id": "I-vZg7LuhwKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function saves (persists) the FAISS index and metadata to disk so that you can use them later without rebuilding from scratch.\n",
        "\n",
        "Step by step\n",
        "\n",
        "Create storage folder if missing\n",
        "os.makedirs(storage_dir, exist_ok=True)\n",
        "Makes sure the folder exists (e.g., \"storage\").\n",
        "If it already exists → no error (because of exist_ok=True).\n",
        "\n",
        "Save the FAISS index\n",
        "faiss.write_index(index, os.path.join(storage_dir, \"faiss.index\"))\n",
        "Writes the FAISS index (all the vectors) to a file called faiss.index.\n",
        "\n",
        "Save the metadata\n",
        "np.save(os.path.join(storage_dir, \"meta.npy\"), np.array(metadata, dtype=object), allow_pickle=True)\n",
        "Converts metadata (Python list of dictionaries) into a NumPy array.\n",
        "Saves it into meta.npy.\n",
        "allow_pickle=True allows saving Python objects (like dictionaries).\n",
        "\n",
        "In simple terms\n",
        "Think of it like saving your progress in a game:\n",
        "The FAISS index is like the game’s world data (your embeddings).\n",
        "The metadata is like the notes telling which vector belongs to which text.\n",
        "This function writes both into files so next time you run the program, you can just load them back instead of starting over."
      ],
      "metadata": {
        "id": "TL6sYG9sF2AY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_texts_to_index(\n",
        "    texts: List[str],\n",
        "    source_name: str,\n",
        "    embedder: SentenceTransformer,\n",
        "    index: faiss.IndexFlatL2,\n",
        "    metadata: List[Dict[str,Any]]\n",
        "):\n",
        "    if not texts:\n",
        "      return\n",
        "    vectors = embedder.encode(texts,convert_to_numpy=True,normalize_embeddings=False)\n",
        "    index.add(vectors)\n",
        "    for i,t in enumerate(texts):\n",
        "      metadata.append({\n",
        "          \"source\": source_name,\n",
        "          \"chunk_id\": i,\n",
        "          \"text\": t\n",
        "      })"
      ],
      "metadata": {
        "id": "2hJWt4ENllyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes text chunks, turns them into embeddings, stores them in FAISS, and records metadata.\n",
        "\n",
        "Step by step\n",
        "\n",
        "Check if texts exist\n",
        "if not texts:\n",
        "    return\n",
        "If there are no texts, just stop.\n",
        "\n",
        "Convert texts → vectors\n",
        "vectors = embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=False)\n",
        "Uses the SentenceTransformer model to convert each text chunk into a numeric vector (embedding).\n",
        "Example: \"Hello world\" → [0.12, -0.34, 0.56, ...]\n",
        "\n",
        "Add vectors to FAISS index\n",
        "index.add(vectors)\n",
        "Stores those embeddings into the FAISS index for similarity search.\n",
        "\n",
        "Save metadata for each chunk\n",
        "for i, t in enumerate(texts):\n",
        "    metadata.append({\n",
        "        \"source\": source_name,\n",
        "        \"chunk_id\": i,\n",
        "        \"text\": t\n",
        "    })\n",
        "Keeps track of:\n",
        "source_name → which file the text came from\n",
        "chunk_id → position number of the chunk\n",
        "text → the actual chunk of text\n",
        "\n",
        "summary.\n",
        "This function is like filing documents in a library:\n",
        "First, it turns the text into a special code (vector) the computer can understand.\n",
        "Then it stores the code in FAISS (like putting it in a drawer).\n",
        "At the same time, it writes a note card (metadata) saying “this code belongs to file X, chunk Y, with this text.”\n",
        "So later, when you search FAISS for similar text, you can also look up the original chunk and where it came from."
      ],
      "metadata": {
        "id": "0u37hJLWUWa6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Retriever\n",
        "def retriever(query: str, embedder,index,metadata,k: int=3):\n",
        "  if index.ntotal == 0:\n",
        "    return []\n",
        "  qvec = embedder.encode([query],convert_to_numpy=True)\n",
        "  D, I = index.search(qvec, k=min(k, index.ntotal))\n",
        "  results=[]\n",
        "  for idx in I[0]:\n",
        "    md = metadata(idx)\n",
        "    results.append(md)\n",
        "  return results"
      ],
      "metadata": {
        "id": "2NHT_HatEPK8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a user’s query, finds the most similar text chunks in the FAISS index, and returns their metadata.\n",
        "\n",
        "Step by step\n",
        "\n",
        "Check if index is empty\n",
        "if index.ntotal == 0:\n",
        "    return []\n",
        "If no data has been added yet, just return an empty list.\n",
        "\n",
        "Convert query → embedding\n",
        "qvec = embedder.encode([query], convert_to_numpy=True)\n",
        "Turns your search question into a vector (same way we did with the text chunks).\n",
        "Example: \"What is AI?\" → [0.22, -0.15, 0.87, ...]\n",
        "\n",
        "Search FAISS for nearest neighbors\n",
        "D, I = index.search(qvec, k=min(k, index.ntotal))\n",
        "index.search finds the k most similar vectors in FAISS.\n",
        "D = distances (how close each result is)\n",
        "I = indices (positions of matching chunks in the metadata list)\n",
        "\n",
        "Collect metadata for results\n",
        "\n",
        "for idx in I[0]:\n",
        "    md = metadata[idx]\n",
        "    results.append(md)\n",
        "\n",
        "\n",
        "Uses the indices I to fetch the corresponding metadata (original text, source file, chunk ID).\n",
        "Builds a list of result dictionaries.\n",
        "Return the matches\n",
        "return results\n",
        "Final output is a list of the most relevant text chunks.\n",
        "\n",
        "summary\n",
        "This function is like asking the library a question:\n",
        "You phrase your question (query).\n",
        "The system translates it into the same “special code” as the stored chunks.\n",
        "FAISS finds the chunks that look closest to your query in vector space.\n",
        "It then gives you the original text + source info (from metadata).\n",
        "\n",
        " Example:\n",
        "results = retrieve(\"What is AI?\", embedder, index, metadata, k=2)\n",
        "\n",
        "Might return:\n",
        "\n",
        "[\n",
        "  {\"source\": \"notes.pdf\", \"chunk_id\": 0, \"text\": \"Artificial Intelligence is...\"},\n",
        "  {\"source\": \"ai_book.docx\", \"chunk_id\": 3, \"text\": \"AI means creating machines...\"}\n",
        "]\n"
      ],
      "metadata": {
        "id": "sfx4hnnUZTdU"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nnC-23brZOfx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}