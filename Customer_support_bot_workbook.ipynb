{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YD6lJRno16M1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlit in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (1.48.1)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (6.2.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (2.3.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (2.3.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (6.32.0)\n",
            "Requirement already satisfied: pyarrow>=7.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (21.0.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (2.32.5)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from streamlit) (6.5.2)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from click<9,>=7.0->streamlit) (0.4.6)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ncYNwM19zEZL"
      },
      "outputs": [],
      "source": [
        "import os   #let's us to interact with computer OS. eg, create/delete folders,read files,check file paths etc...\n",
        "import io   #stands for i/p o/p .Let's us work with data as if it's a file(even if it's in a memory)\n",
        "import re   #Regular expression. helps to find or replace pattern in text\n",
        "import json  #used to work with JSON data(common format to store data like a dict)\n",
        "import pandas as pd  #pandas-->data handling module, popular library for data analysis,works with table(rows and column) like excel\n",
        "import numpy as np  #numpy-->data handling module,library for numerical calculation,great for working with arrays,matrics and math operation\n",
        "import time  #let's us work with time(pause pgm,measure how long something takes)\n",
        "import streamlit as st  #stramlit-->app & dashboard,used to make web apps for DS & ML\n",
        "from typing import List,Dict,Tuple,Any  #type hinting(not code execution,just for clarity),helps to explain datatypes in function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mkkaz4D7TMl"
      },
      "source": [
        "e.g for type hinting\n",
        "\n",
        "def add_numbers(numbers: List[int]) -> int:\n",
        "   return sum(numbers)\n",
        "\n",
        "list[int] means the input is a list of integer\n",
        "-> int means the function returns as integer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "in cursor sentence-transformers are not defaulty installed,so use !pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YF5tUjO011hJ"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import faiss\n",
        "from pypdf import PdfReader\n",
        "from docx import Document as DocxDocument"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frn8O3wn-XoM"
      },
      "source": [
        "NLP & AI models\n",
        "1.from sentence_transformers import SentenceTransformer\n",
        "   library to create sentence embbedding(turns sentence into num vector);useful for sementaic search,recommandation,clustering\n",
        "   Example:\n",
        "   \"I love cats\" → [0.12, -0.55, 0.88, ...] (vector).\n",
        "\n",
        "2.from transformers import pipeline\n",
        "   Hugging Face Transformers library.\n",
        "   pipeline is a shortcut to use pre-trained AI models easily.\n",
        "   Example tasks: text summarization, translation, sentiment analysis, question answering.\n",
        "   Example:\n",
        "   from transformers import pipeline\n",
        "   summarizer = pipeline(\"summarization\")\n",
        "   print(summarizer(\"I love tom\"))\n",
        "\n",
        "Searching Similar Data\n",
        "1.import faiss\n",
        "  FAISS (by Facebook/Meta) is for fast similarity search.\n",
        "  Helps you search quickly through millions of vectors (like sentence embeddings).\n",
        "  Example: find the most similar sentence/document to a query.\n",
        "\n",
        "Reading Files\n",
        "1.from pypdf import PdfReader\n",
        "  Library to read text from PDF files.\n",
        "  Example: load a PDF, get number of pages, extract text.\n",
        "\n",
        "from docx import Document as DocxDocument\n",
        "Library to read and write Word documents (.docx).\n",
        "Example: open a .docx file and read paragraphs, or create a new Word file.\n",
        "\n",
        "\n",
        "In short:\n",
        "\n",
        "SentenceTransformer → turns sentences into numbers (vectors).\n",
        "pipeline → quick way to use AI models (summarize, translate, classify).\n",
        "faiss → finds similar vectors really fast (used for search).\n",
        "PdfReader → extracts text from PDFs.\n",
        "DocxDocument → reads/writes Word docs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W8jzc2ap8pyg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp313-cp313-win_amd64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from faiss-cpu) (2.3.2)\n",
            "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from faiss-cpu) (25.0)\n",
            "Downloading faiss_cpu-1.12.0-cp313-cp313-win_amd64.whl (18.2 MB)\n",
            "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
            "   ------------- -------------------------- 6.0/18.2 MB 30.3 MB/s eta 0:00:01\n",
            "   --------------------------- ------------ 12.6/18.2 MB 29.7 MB/s eta 0:00:01\n",
            "   ---------------------------------------  18.1/18.2 MB 29.3 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 18.2/18.2 MB 24.8 MB/s eta 0:00:00\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5lrWIhnB8wm0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pypdf in c:\\programdata\\anaconda3\\lib\\site-packages (6.0.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pypdf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vACXofJm9RaQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-docx) (5.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "Installing collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install python-docx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Requirement already satisfied: sentence-transformers in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (5.1.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.55.4)\n",
            "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (2.8.0)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (1.7.1)\n",
            "Requirement already satisfied: scipy in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (0.34.4)\n",
            "Requirement already satisfied: Pillow in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.2)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
            "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.11.0->sentence-transformers) (80.9.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\admin\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "lo_99tpw9gcZ"
      },
      "outputs": [],
      "source": [
        "#clean & chunk text\n",
        "def clean_text(text:str)->str:\n",
        "  text=re.sub(r\"\\s+\",\" \",text)\n",
        "  return text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyWYfJp_l3IY"
      },
      "source": [
        "What it does step by step:\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "Defines a function called clean_text.\n",
        "It takes some text as input (text).\n",
        "-> str means it will return a string.\n",
        "\n",
        "re.sub(r\"\\s+\", \" \", text)\n",
        "Uses regular expressions (re).\n",
        "\\s+ means \"one or more spaces, tabs, or newlines\".\n",
        "Replaces them with a single space \" \".\n",
        "Example: \"Hello World\\n\\nHow are you?\" → \"Hello World How are you?\".\n",
        "\n",
        "text.strip()\n",
        "Removes extra spaces at the start and end of the text.\n",
        "Example: \" Hello World \" → \"Hello World\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1CFUGGn6FZ6j"
      },
      "outputs": [],
      "source": [
        "def chunk_text(text:str, chunk_size:int=800, overlap:int=120) -> List[str]:\n",
        "  \"\"\"\n",
        "  Character based chunking(simple & Robust)\n",
        "  Chunk size ~800 chars works well for small model like FLAN-T5\n",
        "  \"\"\"\n",
        "  text = clean_text(text)\n",
        "  chunks = []\n",
        "  start = 0\n",
        "  n = len(text)\n",
        "  while start < n:\n",
        "    end = min(start + chunk_size, n)\n",
        "    chunk = text[start:end]\n",
        "    chunks.append(chunk)\n",
        "    start=end-overlap\n",
        "    if start < 0:\n",
        "      start = 0\n",
        "  return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYdjG-3voEUn"
      },
      "source": [
        "What it does\n",
        "\n",
        "This function splits long text into smaller pieces (chunks).\n",
        "Inputs\n",
        "text: str → the text you want to split.\n",
        "chunk_size: int = 800 → how many characters per chunk (default 800).\n",
        "overlap: int = 120 → how many characters overlap between chunks (default 120).\n",
        "Returns: a list of text chunks (List[str]).\n",
        "\n",
        "text = clean_text(text)\n",
        "Cleans the text (removes extra spaces/newlines).\n",
        "\n",
        "Initialize variables\n",
        "chunks = [] → empty list to store the text pieces.\n",
        "start = 0 → where we start cutting.\n",
        "n = len(text) → total number of characters in text.\n",
        "\n",
        "Loop until we reach the end of the text\n",
        "end = min(start + chunk_size, n)\n",
        "→ defines where the chunk ends.\n",
        "chunk = text[start:end]\n",
        "→ takes a slice of the text.\n",
        "chunks.append(chunk)\n",
        "→ saves the chunk in the list.\n",
        "\n",
        "Overlap handling\n",
        "start = end - overlap\n",
        "→ moves the start backward by 120 chars so the next chunk overlaps with the previous one.\n",
        "This is important because AI models may lose context if chunks don’t overlap.\n",
        "\n",
        "Return chunks\n",
        "After looping, we return the list of all chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je2IrqM7HdE8"
      },
      "outputs": [],
      "source": [
        "#file loader(PDF,DOCX,CSV,TXT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "67VQ7smJI38p"
      },
      "outputs": [],
      "source": [
        "def load_txt(file_bytes: bytes)->str:\n",
        "  return file_bytes.decode(\"utf-8\",errors=\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEeUjr9lo7Xe"
      },
      "source": [
        "What it does\n",
        "\n",
        "file_bytes: bytes\n",
        "The function takes a file that’s been read in bytes form (raw computer data).\n",
        "Example: when you upload a .txt file, it is often read as bytes first.\n",
        "\n",
        ".decode(\"utf-8\", errors=\"ignore\")\n",
        "Converts those bytes into a human-readable string using the utf-8 text format (the most common text encoding).\n",
        "errors=\"ignore\" means: if there are weird symbols it can’t decode, just skip them instead of crashing.\n",
        "\n",
        "Returns\n",
        "A string version of the file’s contents."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "QlEAm7w8JUxY"
      },
      "outputs": [],
      "source": [
        "def load_pdf(file_bytes: bytes)->str:\n",
        "  with io.BytesIO(file_bytes) as fb:\n",
        "    reader = PdfReader(fb)\n",
        "    texts=[]\n",
        "    for page in reader.pages:\n",
        "      try:\n",
        "        t=page.extract_text() or \"\"\n",
        "      except Exception :\n",
        "        t=\"\"\n",
        "      if t:\n",
        "        texts.append(t)\n",
        "  return \"\\n\".join(texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MX78zFknqZz2"
      },
      "source": [
        "What it does\n",
        "\n",
        "This function takes a PDF file in bytes and extracts all the text inside it.\n",
        "Step by step:\n",
        "with io.BytesIO(file_bytes) as fb:\n",
        "Turns the raw file bytes into a file-like object (fb) so that PdfReader can read it (like opening a real file).\n",
        "\n",
        "reader = PdfReader(fb)\n",
        "Creates a PdfReader object to work with the PDF.\n",
        "\n",
        "texts = []\n",
        "An empty list to collect text from each page.\n",
        "\n",
        "Loop through each page\n",
        "for page in reader.pages:\n",
        "Goes page by page inside the PDF.\n",
        "\n",
        "Extract text safely\n",
        "try:\n",
        "    t = page.extract_text() or \"\"\n",
        "except Exception:\n",
        "    t = \"\"\n",
        "page.extract_text() → tries to pull text from the page.\n",
        "or \"\" → if it returns None, replace with an empty string.\n",
        "except → if extraction fails (like scanned PDFs with images), it just skips.\n",
        "\n",
        "Add text if available\n",
        "if t:\n",
        "    texts.append(t)\n",
        "Saves the text from that page into the list.\n",
        "\n",
        "Return joined text\n",
        "return \"\\n\".join(texts)\n",
        "Combines all page texts into one big string, separated by newlines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "0UPYU4Q0UnkM"
      },
      "outputs": [],
      "source": [
        "def load_docx(file_bytes: bytes)->str:\n",
        "  with io.BytesIO(file_bytes) as fb:\n",
        "    doc=DocxDocument(fb)\n",
        "  return \"\\n\".join(p.text for p in doc.paragraphs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2YNMT5TtJ9Y"
      },
      "source": [
        "This function takes a Word file (.docx) in bytes and extracts all the text inside it.\n",
        "\n",
        "Step by step:\n",
        "with io.BytesIO(file_bytes) as fb:\n",
        "Turns the raw file bytes into a file-like object (fb), so Python can treat it like an actual .docx file.\n",
        "\n",
        "doc = DocxDocument(fb)\n",
        "Opens the Word document using the python-docx library.\n",
        "\n",
        "doc.paragraphs\n",
        "Gives you a list of all the paragraphs in the Word document.\n",
        "Each p is a paragraph object, and p.text is its text content.\n",
        "\n",
        "\"/n\".join(p.text for p in doc.paragraphs)\n",
        "Goes through each paragraph (p.text) and joins them together into one big string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "S_dm4tTEZgxR"
      },
      "outputs": [],
      "source": [
        "def load_csv(file_bytes: bytes)->str:\n",
        "  with io.BytesIO(file_bytes) as fb:\n",
        "    df=pd.read_csv(fb)\n",
        "    #converting to readable FAQ-like table text\n",
        "  return df.to_csv(index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puMi-Cdut_qk"
      },
      "source": [
        "This function takes a CSV file in bytes and converts it into a text format (CSV string).\n",
        "\n",
        "Step by step:\n",
        "with io.BytesIO(file_bytes) as fb:\n",
        "Converts the raw bytes into a file-like object (fb) so pandas can read it.\n",
        "\n",
        "df = pd.read_csv(fb)\n",
        "Reads the CSV file into a pandas DataFrame (like an Excel table in Python).\n",
        "\n",
        "Comment: # converting to readable FAQ-like table text\n",
        "This means the idea is to convert the data into a nice text format (like Q&A style), but in this code they just output it back as CSV text.\n",
        "\n",
        "return df.to_csv(index=False)\n",
        "Converts the DataFrame back into a CSV string, without the row index.\n",
        "Basically, it gives you the CSV contents as plain text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "bS6DobL1aXrX"
      },
      "outputs": [],
      "source": [
        "def read_any(file)->Tuple[str,str]:\n",
        "  name=file.name.lower()\n",
        "  content=file.read()\n",
        "  if name.endswith(\".pdf\"):\n",
        "    return \"pdf\",load_pdf(content)\n",
        "  elif name.endswith(\".docx\"):\n",
        "    return \"docx\",load_docx(content)\n",
        "  elif name.endswith(\".csv\"):\n",
        "    return \"csv\",load_csv(content)\n",
        "  elif name.endswith(\".txt\"):\n",
        "    return \"txt\",load_txt(content)\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported file type.Please upload PDF,DOCX,TXT, OR CSV.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cll5Vdahu9qF"
      },
      "source": [
        "This function can read different types of files (PDF, DOCX, CSV, TXT) using the right loader function.\n",
        "\n",
        "Step by step:\n",
        "name = file.name.lower()\n",
        "Gets the file name (like \"example.PDF\").\n",
        "Converts it to lowercase so it’s easier to check extensions (e.g., \"pdf\" vs \"PDF\").\n",
        "\n",
        "content = file.read()\n",
        "Reads the file content into memory (as bytes).\n",
        "\n",
        "Check the file type using extension\n",
        "If the file ends with .pdf → call load_pdf(content)\n",
        "If .docx → call load_docx(content)\n",
        "If .csv → call load_csv(content)\n",
        "If .txt → call load_txt(content)\n",
        "\n",
        "Return a tuple\n",
        "First item = file type (like \"pdf\", \"docx\", etc.).\n",
        "Second item = the extracted text from the file.\n",
        "\n",
        "Unsupported file\n",
        "If the file type isn’t recognized, it raises an error telling the user only PDF, DOCX, TXT, or CSV are allowed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pYiSxnPed-VL"
      },
      "outputs": [],
      "source": [
        "#Embedding + FAISS handling\n",
        "@st.cache_resource\n",
        "def get_embedder():\n",
        "  return SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKsyDRIww7nD"
      },
      "source": [
        "What it does\n",
        "\n",
        "@st.cache_resource\n",
        "This is a Streamlit decorator.\n",
        "It tells Streamlit:\n",
        "“Run this function only once and cache (remember) the result.”\n",
        "So if you call get_embedder() many times in the app, it won’t reload the model every time (saves time).\n",
        "\n",
        "def get_embedder():\n",
        "Defines a function named get_embedder.\n",
        "\n",
        "SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "Loads a pretrained embedding model from sentence-transformers.\n",
        "Model name: all-MiniLM-L6-v2 (a small, fast, but good model for embeddings).\n",
        "This model converts sentences into embeddings (numeric vectors).\n",
        "Example: \"Hello world\" → [0.23, -0.51, 0.88, ...]\n",
        "\n",
        "return SentenceTransformer(...)\n",
        "The function returns the model object.\n",
        "So now, whenever you need embeddings, you just call:\n",
        "embedder = get_embeder()\n",
        "vectors=embedder.encode([\"hi sam\",\"how u doing?\"])\n",
        "\n",
        "summary:\n",
        "This function loads a text embedding model once, caches it, and reuses it.\n",
        "The model converts sentences → vectors (embeddings).\n",
        "These embeddings are later used with FAISS for similarity search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Krd2u4uvels-"
      },
      "outputs": [],
      "source": [
        "def build_or_load_index(\n",
        "    embedder:SentenceTransformer,\n",
        "    storage_dir:str=\"storage\"\n",
        ") ->Tuple[faiss.IndexFlatL2,List[dict[str,Any]]]:\n",
        "    os.makedirs(storage_dir,exist_ok=True)\n",
        "    index_path=os.path.join(storage_dir,\"faiss.index\")\n",
        "    meta_path=os.path.join(storage_dir,\"meta.npy\")\n",
        "\n",
        "    if os.path.exists(index_path) and os.path.exists(meta_path):\n",
        "        index=faiss.read_index(index_path)\n",
        "        metadata=np.load(meta_path,allow_pickle=True).tolist()\n",
        "    return index,metadata\n",
        "\n",
        "    #Empty new index\n",
        "    index=faiss.IndexFlatL2(384)\n",
        "    metadata: List[Dict[str,Any]]=[]\n",
        "    return index,metadata\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSCmZ0kOzq-S"
      },
      "source": [
        "This function either loads an existing FAISS index (and metadata) from disk or, if none exists, creates a new empty index.\n",
        "\n",
        "Step by step:\n",
        "Function definition\n",
        "Takes in an embedder (the SentenceTransformer model).\n",
        "storage_dir=\"storage\" → default folder where index files are saved.\n",
        "Returns:\n",
        "a FAISS index (faiss.IndexFlatL2)\n",
        "metadata (list of dictionaries with extra info about each chunk).\n",
        "\n",
        "Paths for storage\n",
        "index_path = os.path.join(storage_dir, \"faiss.index\")\n",
        "meta_path  = os.path.join(storage_dir, \"meta.npy\")\n",
        "\n",
        "One file for the FAISS index.\n",
        "One file for the metadata.\n",
        "\n",
        "Check if files exist\n",
        "if os.path.exists(index_path) and os.path.exists(meta_path):\n",
        "    index = faiss.read_index(index_path)        # load FAISS index\n",
        "    metadata = np.load(meta_path, allow_pickle=True).tolist()  # load metadata\n",
        "    return index, metadata\n",
        "If both files are present → load them from disk and return.\n",
        "\n",
        "If no files found → create a new index\n",
        "index = faiss.IndexFlatL2(384)\n",
        "metadata: List[Dict[str, Any]] = []\n",
        "return index, metadata\n",
        "Creates a new FAISS index with vectors of size 384 (because \"all-MiniLM-L6-v2\" embeddings are 384-dimensional).\n",
        "Starts with an empty metadata list.\n",
        "\n",
        "Example usage\n",
        "embedder = get_embedder()\n",
        "index, metadata = build_or_load_index(embedder)\n",
        "\n",
        "print(index.ntotal)   # number of vectors stored\n",
        "print(len(metadata))  # number of metadata entries\n",
        "\n",
        "\n",
        "If nothing exists yet → you get an empty FAISS index.\n",
        "If files exist → it resumes from saved data.\n",
        "\n",
        "In simple terms:\n",
        "This function makes sure you always have a FAISS index ready:\n",
        "If you’ve already built and saved one → it loads it.\n",
        "If not → it creates a new, empty one.\n",
        "\n",
        "What is a FAISS index?\n",
        "\n",
        "FAISS = Facebook AI Similarity Search.\n",
        "A FAISS index is like a special database designed to store vectors (embeddings) and let you quickly find the ones that are most similar to a query.\n",
        "\n",
        "Think of it as:\n",
        "You turn sentences into number vectors using SentenceTransformer.\n",
        "You put those vectors inside a FAISS index.\n",
        "Later, when you give a new query (also turned into a vector), FAISS searches the index and finds the closest vectors (i.e., most similar sentences).\n",
        "So instead of searching text directly, you’re searching based on semantic meaning.\n",
        "\n",
        "What is faiss.IndexFlatL2?\n",
        "\n",
        "FAISS provides different types of indexes.\n",
        "IndexFlatL2 means:\n",
        "Flat → It stores all vectors directly (no fancy compression, no approximation).\n",
        "L2 → It measures similarity using L2 distance (Euclidean distance).\n",
        "That’s just the usual \"straight-line distance\" in multi-dimensional space.\n",
        "\n",
        "sumaary:\n",
        "FAISS index = a smart database for embeddings.\n",
        "IndexFlatL2 = the simplest type of FAISS index, which stores vectors and compares them using Euclidean distance to find similar ones.\n",
        "\n",
        "Later, you can use more advanced FAISS indexes (IndexIVF, HNSW, etc.) for faster searches when you have millions of vectors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "I-vZg7LuhwKw"
      },
      "outputs": [],
      "source": [
        "def persist_index(index: faiss.IndexFlatL2, metadata: List[Dict[str, Any]], storage_dir: str = \"storage\"):\n",
        "    os.makedirs(storage_dir, exist_ok=True)\n",
        "    faiss.write_index(index, os.path.join(storage_dir, \"faiss.index\"))\n",
        "    np.save(os.path.join(storage_dir, \"meta.npy\"), np.array(metadata, dtype=object), allow_pickle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TL6sYG9sF2AY"
      },
      "source": [
        "This function saves (persists) the FAISS index and metadata to disk so that you can use them later without rebuilding from scratch.\n",
        "\n",
        "Step by step\n",
        "\n",
        "Create storage folder if missing\n",
        "os.makedirs(storage_dir, exist_ok=True)\n",
        "Makes sure the folder exists (e.g., \"storage\").\n",
        "If it already exists → no error (because of exist_ok=True).\n",
        "\n",
        "Save the FAISS index\n",
        "faiss.write_index(index, os.path.join(storage_dir, \"faiss.index\"))\n",
        "Writes the FAISS index (all the vectors) to a file called faiss.index.\n",
        "\n",
        "Save the metadata\n",
        "np.save(os.path.join(storage_dir, \"meta.npy\"), np.array(metadata, dtype=object), allow_pickle=True)\n",
        "Converts metadata (Python list of dictionaries) into a NumPy array.\n",
        "Saves it into meta.npy.\n",
        "allow_pickle=True allows saving Python objects (like dictionaries).\n",
        "\n",
        "In simple terms\n",
        "Think of it like saving your progress in a game:\n",
        "The FAISS index is like the game’s world data (your embeddings).\n",
        "The metadata is like the notes telling which vector belongs to which text.\n",
        "This function writes both into files so next time you run the program, you can just load them back instead of starting over."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "2hJWt4ENllyk"
      },
      "outputs": [],
      "source": [
        "def add_texts_to_index(\n",
        "    texts: List[str],\n",
        "    source_name: str,\n",
        "    embedder: SentenceTransformer,\n",
        "    index: faiss.IndexFlatL2,\n",
        "    metadata: List[Dict[str,Any]]\n",
        "):\n",
        "    if not texts:\n",
        "      return\n",
        "    vectors = embedder.encode(texts,convert_to_numpy=True,normalize_embeddings=False)\n",
        "    index.add(vectors)\n",
        "    for i,t in enumerate(texts):\n",
        "      metadata.append({\n",
        "          \"source\": source_name,\n",
        "          \"chunk_id\": i,\n",
        "          \"text\": t\n",
        "      })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0u37hJLWUWa6"
      },
      "source": [
        "This function takes text chunks, turns them into embeddings, stores them in FAISS, and records metadata.\n",
        "\n",
        "Step by step\n",
        "\n",
        "Check if texts exist\n",
        "if not texts:\n",
        "    return\n",
        "If there are no texts, just stop.\n",
        "\n",
        "Convert texts → vectors\n",
        "vectors = embedder.encode(texts, convert_to_numpy=True, normalize_embeddings=False)\n",
        "Uses the SentenceTransformer model to convert each text chunk into a numeric vector (embedding).\n",
        "Example: \"Hello world\" → [0.12, -0.34, 0.56, ...]\n",
        "\n",
        "Add vectors to FAISS index\n",
        "index.add(vectors)\n",
        "Stores those embeddings into the FAISS index for similarity search.\n",
        "\n",
        "Save metadata for each chunk\n",
        "for i, t in enumerate(texts):\n",
        "    metadata.append({\n",
        "        \"source\": source_name,\n",
        "        \"chunk_id\": i,\n",
        "        \"text\": t\n",
        "    })\n",
        "Keeps track of:\n",
        "source_name → which file the text came from\n",
        "chunk_id → position number of the chunk\n",
        "text → the actual chunk of text\n",
        "\n",
        "summary.\n",
        "This function is like filing documents in a library:\n",
        "First, it turns the text into a special code (vector) the computer can understand.\n",
        "Then it stores the code in FAISS (like putting it in a drawer).\n",
        "At the same time, it writes a note card (metadata) saying “this code belongs to file X, chunk Y, with this text.”\n",
        "So later, when you search FAISS for similar text, you can also look up the original chunk and where it came from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "2NHT_HatEPK8"
      },
      "outputs": [],
      "source": [
        "#Retriever\n",
        "def retriever(query: str, embedder,index,metadata,k: int=3):\n",
        "  if index.ntotal == 0:\n",
        "    return []\n",
        "  qvec = embedder.encode([query],convert_to_numpy=True)\n",
        "  D, I = index.search(qvec, k=min(k, index.ntotal))\n",
        "  results=[]\n",
        "  for idx in I[0]:\n",
        "    md = metadata(idx)\n",
        "    results.append(md)\n",
        "  return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfx4hnnUZTdU"
      },
      "source": [
        "This function takes a user’s query, finds the most similar text chunks in the FAISS index, and returns their metadata.\n",
        "\n",
        "Step by step\n",
        "\n",
        "Check if index is empty\n",
        "if index.ntotal == 0:\n",
        "    return []\n",
        "If no data has been added yet, just return an empty list.\n",
        "\n",
        "Convert query → embedding\n",
        "qvec = embedder.encode([query], convert_to_numpy=True)\n",
        "Turns your search question into a vector (same way we did with the text chunks).\n",
        "Example: \"What is AI?\" → [0.22, -0.15, 0.87, ...]\n",
        "\n",
        "Search FAISS for nearest neighbors\n",
        "D, I = index.search(qvec, k=min(k, index.ntotal))\n",
        "index.search finds the k most similar vectors in FAISS.\n",
        "D = distances (how close each result is)\n",
        "I = indices (positions of matching chunks in the metadata list)\n",
        "\n",
        "Collect metadata for results\n",
        "\n",
        "for idx in I[0]:\n",
        "    md = metadata[idx]\n",
        "    results.append(md)\n",
        "\n",
        "\n",
        "Uses the indices I to fetch the corresponding metadata (original text, source file, chunk ID).\n",
        "Builds a list of result dictionaries.\n",
        "Return the matches\n",
        "return results\n",
        "Final output is a list of the most relevant text chunks.\n",
        "\n",
        "summary\n",
        "This function is like asking the library a question:\n",
        "You phrase your question (query).\n",
        "The system translates it into the same “special code” as the stored chunks.\n",
        "FAISS finds the chunks that look closest to your query in vector space.\n",
        "It then gives you the original text + source info (from metadata).\n",
        "\n",
        " Example:\n",
        "results = retrieve(\"What is AI?\", embedder, index, metadata, k=2)\n",
        "\n",
        "Might return:\n",
        "\n",
        "[\n",
        "  {\"source\": \"notes.pdf\", \"chunk_id\": 0, \"text\": \"Artificial Intelligence is...\"},\n",
        "  {\"source\": \"ai_book.docx\", \"chunk_id\": 3, \"text\": \"AI means creating machines...\"}\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nnC-23brZOfx"
      },
      "outputs": [],
      "source": [
        "#Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "c5nUrww1_81E"
      },
      "outputs": [],
      "source": [
        "@st.cache_resource\n",
        "def get_generator():\n",
        "  return pipeline(\"text2text-generation\",model=\"google/flan-t5-base\") #flan-t5 is light and fine for grounded ans."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "hAOb3P5wAmTX"
      },
      "outputs": [],
      "source": [
        "def synthesize_answer(user_query: str,contexts: List[Dict[str,Any]],history: List[Dict[str,str]],generator) -> str:\n",
        "  conv = \"\\n\".join([f\"{m['role'].capitalize()}:{m['content']}\" for m in history[-6:]])\n",
        "  context_text = \"\\n\\n\".join([f\"[{i+1}] {c['text']}\" for i, c in enumerate(contexts)]) if contexts else \"No context\"\n",
        "  prompt = f\"\"\"You are a helpful, concise support assistant. Answer the final user question using ONLY the provided context. If missing, say you don't know.\n",
        "Coversation so far:\n",
        "{conv}\n",
        "\n",
        "context:\n",
        "{context_text}\n",
        "\n",
        "Final user question: {user_query}\n",
        "Answer:\"\"\"\n",
        "  out = generator(prompt,max_length=220,num_return_sequences=1)[0][\"generated_text\"]\n",
        "  return out.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKNlCKNkLBgp"
      },
      "source": [
        "This function takes the user’s question, adds context from documents, adds chat history, and asks the AI model (FLAN-T5) to generate an answer.\n",
        "\n",
        "Step by step\n",
        "1. Get the AI model\n",
        "return pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
        "Uses FLAN-T5, a lightweight model good at answering questions with context.\n",
        "pipeline(\"text2text-generation\") means: given a text prompt, generate an output text.\n",
        "\n",
        "2. Collect conversation history\n",
        "convo = \"\\n\".join([f\"{m['role'].capitalize()}: {m['content']}\" for m in history[-6:]])\n",
        "Formats the last 6 messages from the conversation.\n",
        "Example:\n",
        "User: What is AI?\n",
        "Assistant: AI means...\n",
        "\n",
        "3. Collect relevant context\n",
        "context_text = \"\\n\\n\".join([f\"[{i+1}] {c['text']}\" for i, c in enumerate(contexts)]) if contexts else \"No context.\"\n",
        "Joins together the retrieved chunks (from FAISS).\n",
        "Example:\n",
        "[1] Artificial Intelligence is the field of...\n",
        "[2] AI can be used in...\n",
        "\n",
        "4. Build the prompt\n",
        "prompt = f\"\"\"You are a helpful, concise support assistant...\n",
        "Combines:\n",
        "The conversation history (convo)\n",
        "The context from documents (context_text)\n",
        "The user’s latest question (user_query)\n",
        "It also instructs the model:\n",
        "“Use ONLY the context. If not found, say you don’t know.”\n",
        "\n",
        "5. Generate the answer\n",
        "out = generator(prompt, max_length=220, num_return_sequences=1)[0][\"generated_text\"]\n",
        "Sends the prompt to the model (generator).\n",
        "Limits answer length to 220 tokens.\n",
        "Returns one best answer.\n",
        "\n",
        "6. Return clean output\n",
        "return out.strip()\n",
        "Strips extra spaces/newlines.\n",
        "\n",
        "summary\n",
        "This function is like the final stage of a Q&A assistant:\n",
        "Gather what the user asked before (chat history).\n",
        "Gather useful text from the documents (context).\n",
        "Create a clear instruction prompt.\n",
        "Ask the AI model (FLAN-T5) to generate a short, helpful answer.\n",
        "Return that as the assistant’s reply.\n",
        "\n",
        "Example:\n",
        "query = \"What is AI?\"\n",
        "contexts = [{\"text\": \"Artificial Intelligence is the science of making machines smart.\"}]\n",
        "history = [{\"role\": \"user\", \"content\": \"Hello\"}, {\"role\": \"assistant\", \"content\": \"Hi, how can I help?\"}]\n",
        "generator = get_generator()\n",
        "\n",
        "print(synthesize_answer(query, contexts, history, generator))\n",
        "\n",
        "Might output:\n",
        "\"AI is the science of making machines smart, as described in the context.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "YOmDxkVEKUhQ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-08-26 14:46:47.669 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-26 14:46:47.673 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-26 14:46:48.007 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run c:\\ProgramData\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
            "2025-08-26 14:46:48.009 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-26 14:46:48.010 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-26 14:46:48.014 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-26 14:46:48.016 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
            "2025-08-26 14:46:48.017 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DeltaGenerator()"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#streamlit app\n",
        "st.set_page_config(page_title=\"AI Support Assistant (Multi-doc, Local)\",page_icon=\"🤖\",layout=\"wide\")\n",
        "st.title(\"🤖 AI-Powered Customer Support Assistant\")\n",
        "st.caption(\"Runs locally • No API keys • Multi-document RAG • Citations • Session memory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
        "DeltaGenerator()\n",
        "is from Streamlit.\n",
        "\n",
        "Let me explain in simple terms:\n",
        "Why it happens\n",
        "Streamlit code (like st.title(), st.caption(), st.write(), etc.) is meant to run inside a Streamlit app (when you launch it using streamlit run your_app.py).\n",
        "\n",
        "If you just run the script in a normal Python environment (like Jupyter Notebook, Colab, or plain python script.py), Streamlit doesn’t have its \"context\" (the special environment it needs to show things in the web app).\n",
        "\n",
        "That’s why it prints this warning: “missing ScriptRunContext! This warning can be ignored when running in bare mode.”\n",
        "\n",
        "In plain English → Streamlit is saying: “Hey, you’re not running me in app mode, so I can’t actually show UI. I’ll just return an empty object instead.”\n",
        "\n",
        "How to fix / use properly\n",
        "If you actually want to see the app UI, you need to run:\n",
        "streamlit run your_app.py\n",
        "This will open a local web page (usually at http://localhost:8501) with your assistant interface.\n",
        "\n",
        "Inside Jupyter/Colab, Streamlit calls will not render the UI — they just return placeholders (DeltaGenerator() objects).\n",
        "\n",
        "In simple terms\n",
        "The warning is not an error.\n",
        "It just means: “I’m in notebook/python mode, not in Streamlit app mode.”\n",
        "To really use the chatbot app, you must run it with Streamlit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In cursor ai app, first i given pip install streamlit in this cursor ai terminal, it succefully executed but when i give import streamlit as st, it's showing error\n",
        "\n",
        "I can see what went wrong.\n",
        "You tried this in the terminal:\n",
        "import streamlit as st\n",
        "But that is Python code, not a terminal command. That’s why PowerShell is saying:\n",
        "import : The term 'import' is not recognized...\n",
        "\n",
        "Correct way:\n",
        "\n",
        "First, make sure Streamlit is installed (you already did this):\n",
        "pip install streamlit\n",
        "Then, instead of running import streamlit as st in the terminal,\n",
        "you need to run your app with Streamlit like this:\n",
        "streamlit run chatbot_st.py\n",
        "(replace chatbot_st.py with your filename)\n",
        "\n",
        "That will open a local webpage in your browser (usually http://localhost:8501) with your app UI.\n",
        "\n",
        "What not to do\n",
        "Don’t type import streamlit as st in the terminal — it only works inside your Python script.\n",
        "\n",
        "Example inside chatbot_st.py:\n",
        "import streamlit as st\n",
        "st.title(\"🤖 AI-Powered Customer Support Assistant\")\n",
        "Try running:\n",
        "streamlit run chatbot_st.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PS C:\\Users\\Admin\\Documents\\CusorAIDoc> streamlit run .\\chatbot_st.py streamlit : The term 'streamlit' is not recognized as the name of a cmdlet, function, script file, or operable program. Check the spelling of the name, or if a path was included, verify that the path is correct and try again. At line:1 char:1 + streamlit run .\\chatbot_st.py + ~~~~~~~~~ + CategoryInfo : ObjectNotFound: (streamlit:String) [], CommandNotFoundException + FullyQualifiedErrorId : CommandNotFoundException\n",
        "\n",
        "\n",
        "PowerShell can’t find the streamlit command, which means your PATH/environment is not set up correctly.\n",
        "Even though pip install streamlit worked, the streamlit.exe script is not in your PATH.\n",
        "\n",
        "Step 1: Verify installation\n",
        "Run this in the same terminal:\n",
        "python -m pip show streamlit\n",
        "If it shows details (version, location, etc.), then Streamlit is installed, but the command shortcut (streamlit) is not linked to PATH.\n",
        "\n",
        "Step 2: Run Streamlit without PATH issues\n",
        "Instead of streamlit run ..., run it through Python directly:\n",
        "python -m streamlit run chatbot_st.py\n",
        "That forces Python to use the installed Streamlit module.\n",
        "\n",
        "Step 3 (Optional Fix for PATH)\n",
        "If you want the streamlit command to work globally:\n",
        "Find where scripts are installed:\n",
        "python -m site --user-base\n",
        "It will return a path like:\n",
        "C:\\Users\\Admin\\AppData\\Roaming\\Python\\Python313\n",
        "Inside it, there’s a Scripts\\ folder containing streamlit.exe.\n",
        "Add that Scripts\\ folder to your PATH in Windows environment variables.\n",
        "\n",
        "For now, the quickest way is:\n",
        "python -m streamlit run chatbot_st.py\n",
        "\n",
        "\n",
        "Do you want me to walk you through how to permanently fix the PATH so you can just type streamlit run ... next time?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#don't execute it\n",
        "st.set_page_config(\n",
        "    page_title=\"AI Support Assistant (Multi-doc, Local)\",\n",
        "    page_icon=\"🤖\",\n",
        "    layout=\"wide\"\n",
        ")\n",
        "st.title(\"🤖 AI-Powered Customer Support Assistant\")\n",
        "st.caption(\"Runs locally • No API keys • Multi-document RAG • Citations • Session memory\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What each line does\n",
        "\n",
        "st.set_page_config(...)\n",
        "Configures the overall page in Streamlit.\n",
        "page_title=\"AI Support Assistant (Multi-doc, Local)\" → The title shown in the browser tab.\n",
        "page_icon=\"🤖\" → The favicon (little emoji in tab bar).\n",
        "layout=\"wide\" → Makes the app stretch to the full width of the screen (instead of narrow center).\n",
        "\n",
        "st.title(\"🤖 AI-Powered Customer Support Assistant\")\n",
        "Displays a big headline at the top of the app.\n",
        "In this case:\n",
        "🤖 AI-Powered Customer Support Assistant\n",
        "\n",
        "st.caption(\"Runs locally • No API keys • Multi-document RAG • Citations • Session memory\")\n",
        "Adds a smaller subtitle / description under the title.\n",
        "Tells the user key features of the app:\n",
        "Runs locally (no internet needed for model calls)\n",
        "No API keys required\n",
        "Can handle multiple documents (RAG = Retrieval Augmented Generation)\n",
        "Gives citations (where the answer came from)\n",
        "Remembers session chat history\n",
        "\n",
        "summary:\n",
        "This block is just about making your app look professional when you open it:\n",
        "Sets tab title + icon\n",
        "Shows a big app title\n",
        "Shows a short description of what it can do"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#don't execute\n",
        "# Side: model & index controls\n",
        "with st.sidebar:\n",
        "    st.subheader(\"📁 Knowledge Base\")\n",
        "    st.write(\"Upload PDFs, DOCX, TXT, CSV. They’ll be embedded locally and indexed.\")\n",
        "\n",
        "    uploaded_files = st.file_uploader(\n",
        "        \"Add documents\",\n",
        "        type=[\"pdf\", \"docx\", \"txt\", \"csv\"],\n",
        "        accept_multiple_files=True\n",
        "    )\n",
        "    paste_text = st.text_area(\"Or paste text (optional):\", height=120, placeholder=\"Paste product manuals, FAQs, SOPs...\")\n",
        "\n",
        "    chunk_size = st.slider(\"Chunk size (chars)\", 400, 1500, 800, 50)\n",
        "    overlap = st.slider(\"Overlap (chars)\", 50, 400, 120, 10)\n",
        "    top_k = st.slider(\"Citations (top-K)\", 1, 5, 3, 1)\n",
        "\n",
        "\n",
        "    # Load components\n",
        "    embedder = get_embedder()\n",
        "    generator = get_generator()\n",
        "    index, metadata = build_or_load_index(embedder)\n",
        "\n",
        "    if st.button(\"➕ Ingest now\", use_container_width=True):\n",
        "        new_texts = []\n",
        "\n",
        "        # From files\n",
        "        for f in uploaded_files or []:\n",
        "            try:\n",
        "                ftype, raw = read_any(f)\n",
        "                chunks = chunk_text(raw, chunk_size=chunk_size, overlap=overlap)\n",
        "                new_texts.extend([(f.name, c) for c in chunks])\n",
        "            except Exception as e:\n",
        "                st.error(f\"Failed to read {f.name}: {e}\")\n",
        "\n",
        "        # From pasted text\n",
        "        if paste_text.strip():\n",
        "            chunks = chunk_text(paste_text, chunk_size=chunk_size, overlap=overlap)\n",
        "            new_texts.extend([(\"pasted_text\", c) for c in chunks])\n",
        "\n",
        "        if new_texts:\n",
        "            # Batch by source for cleaner metadata\n",
        "            by_source: Dict[str, List[str]] = {}\n",
        "            for src, c in new_texts:\n",
        "                by_source.setdefault(src, []).append(c)\n",
        "\n",
        "            total_added = 0\n",
        "            for src, chunks in by_source.items():\n",
        "                add_texts_to_index(chunks, src, embedder, index, metadata)\n",
        "                total_added += len(chunks)\n",
        "\n",
        "            persist_index(index, metadata)\n",
        "            st.success(f\"Ingested {total_added} chunks from {len(by_source)} sources 👍\")\n",
        "        else:\n",
        "            st.info(\"No new content to ingest.\")\n",
        "\n",
        "    if st.button(\"🧹 Reset index\", type=\"secondary\", use_container_width=True):\n",
        "        # wipe storage\n",
        "        try:\n",
        "            os.remove(os.path.join(\"storage\", \"faiss.index\"))\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "        try:\n",
        "            os.remove(os.path.join(\"storage\", \"meta.npy\"))\n",
        "        except FileNotFoundError:\n",
        "            pass\n",
        "        st.experimental_rerun()\n",
        "\n",
        "    st.markdown(\"---\")\n",
        "    st.write(\"**Index size:**\", getattr(index, \"ntotal\", 0))\n",
        "    if os.path.exists(\"data/faqs.txt\"):\n",
        "        st.write(\"Tip: add your starter FAQs in `data/faqs.txt` and ingest them too.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code is creating a sidebar in your Streamlit app where the user can control the knowledge base and how documents are processed.\n",
        "\n",
        "Line by line:\n",
        "with st.sidebar:\n",
        "Everything inside this block will appear on the sidebar of the app (left side).\n",
        "\n",
        "st.subheader(\"📁 Knowledge Base\")\n",
        "st.write(\"Upload PDFs, DOCX, TXT, CSV. They’ll be embedded locally and indexed.\")\n",
        "Adds a title and a description to explain what the sidebar is for.\n",
        "It tells the user they can upload documents that will be turned into embeddings and indexed (so the AI can search them).\n",
        "\n",
        "uploaded_files = st.file_uploader(\n",
        "    \"Add documents\",\n",
        "    type=[\"pdf\", \"docx\", \"txt\", \"csv\"],\n",
        "    accept_multiple_files=True\n",
        ")\n",
        "\n",
        "This creates a file uploader button.\n",
        "Users can upload multiple files (PDF, Word, TXT, CSV).\n",
        "Those files will be stored in uploaded_files.\n",
        "\n",
        "paste_text = st.text_area(\"Or paste text (optional):\", height=120, placeholder=\"Paste product manuals, FAQs, SOPs...\")\n",
        "This gives another option: instead of uploading files, users can paste raw text (like FAQs or manuals).\n",
        "That text will go into paste_text.\n",
        "\n",
        "chunk_size = st.slider(\"Chunk size (chars)\", 400, 1500, 800, 50)\n",
        "Chunk size slider:\n",
        "When we embed documents, they’re too long for the model to handle at once.\n",
        "So we break them into smaller pieces (\"chunks\").\n",
        "This slider lets the user pick the chunk size (default = 800 characters, min = 400, max = 1500).\n",
        "\n",
        "overlap = st.slider(\"Overlap (chars)\", 50, 400, 120, 10)\n",
        "Overlap slider:\n",
        "When splitting text into chunks, some text is repeated between chunks (overlap).\n",
        "This helps the AI maintain context between chunks.\n",
        "Here, default is 120 characters overlap.\n",
        "\n",
        "top_k = st.slider(\"Citations (top-K)\", 1, 5, 3, 1)\n",
        "Top-K slider:\n",
        "This controls how many top relevant chunks (citations) are retrieved when answering a query.\n",
        "For example, if top_k=3, it will pull the 3 most relevant text chunks to answer.\n",
        "\n",
        "In summary:\n",
        "\n",
        "This sidebar is like a control panel where the user can:\n",
        "Upload files 📂\n",
        "Paste text ✍️\n",
        "Adjust how documents are split (chunk size + overlap)\n",
        "Control how many citations to show in answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From load components to wipe storage -> This is the part of your app that actually loads AI models, ingests documents, and manages the index.\n",
        "\n",
        "Step by Step:\n",
        "embedder = get_embedder()\n",
        "generator = get_generator()\n",
        "index, metadata = build_or_load_index(embedder)\n",
        "\n",
        "Loads:\n",
        "embedder = model that converts text into embeddings (vectors).\n",
        "generator = model that answers questions.\n",
        "index, metadata = either builds a new FAISS index (vector database) or loads an existing one.\n",
        "\n",
        "if st.button(\"➕ Ingest now\", use_container_width=True):\n",
        "When user clicks “Ingest now”, we start processing new documents or pasted text\n",
        "\n",
        "📂 From uploaded files:\n",
        "for f in uploaded_files or []:\n",
        "    try:\n",
        "        ftype, raw = read_any(f)\n",
        "        chunks = chunk_text(raw, chunk_size=chunk_size, overlap=overlap)\n",
        "        new_texts.extend([(f.name, c) for c in chunks])\n",
        "    except Exception as e:\n",
        "        st.error(f\"Failed to read {f.name}: {e}\")\n",
        "\n",
        "For each uploaded file:\n",
        "Read its content (read_any).\n",
        "Break into chunks (chunk_text).\n",
        "Add chunks to new_texts.\n",
        "  If something goes wrong, show an error.\n",
        "\n",
        "  ✍️ From pasted text:\n",
        "if paste_text.strip():\n",
        "    chunks = chunk_text(paste_text, chunk_size=chunk_size, overlap=overlap)\n",
        "    new_texts.extend([(\"pasted_text\", c) for c in chunks])\n",
        "\n",
        "If user typed/pasted something in the text area:\n",
        "Break into chunks.\n",
        "Add them as \"pasted_text\" source.\n",
        "\n",
        "🗂 Organize by source:\n",
        "by_source: Dict[str, List[str]] = {}\n",
        "for src, c in new_texts:\n",
        "    by_source.setdefault(src, []).append(c)\n",
        "Groups chunks by file name (or \"pasted_text\").\n",
        "Example:\n",
        "manual.pdf → [chunk1, chunk2, ...]\n",
        "faq.docx → [chunk1, chunk2, ...]\n",
        "\n",
        "Add to index:\n",
        "for src, chunks in by_source.items():\n",
        "    add_texts_to_index(chunks, src, embedder, index, metadata)\n",
        "    total_added += len(chunks)\n",
        "For each source:\n",
        "Convert chunks into embeddings using embedder.\n",
        "Store them in index along with metadata.\n",
        "\n",
        "Save index:\n",
        "persist_index(index, metadata)\n",
        "st.success(f\"Ingested {total_added} chunks from {len(by_source)} sources 👍\")\n",
        "Saves the FAISS index + metadata to disk.\n",
        "Shows success message.\n",
        "\n",
        "❌ If no new content:\n",
        "else:\n",
        "    st.info(\"No new content to ingest.\")\n",
        "Tells user nothing was added.\n",
        "\n",
        "🧹 Reset index:\n",
        "if st.button(\"🧹 Reset index\", type=\"secondary\", use_container_width=True):\n",
        "    # wipe storage\n",
        "    try:\n",
        "        os.remove(os.path.join(\"storage\", \"faiss.index\"))\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "    try:\n",
        "        os.remove(os.path.join(\"storage\", \"meta.npy\"))\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "    st.experimental_rerun()\n",
        "\n",
        "If user clicks “Reset index”:\n",
        "Deletes stored FAISS index + metadata.\n",
        "Restarts the app with an empty index.\n",
        "\n",
        "Show index info:\n",
        "st.markdown(\"---\")\n",
        "st.write(\"**Index size:**\", getattr(index, \"ntotal\", 0))\n",
        "if os.path.exists(\"data/faqs.txt\"):\n",
        "    st.write(\"Tip: add your starter FAQs in `data/faqs.txt` and ingest them too.\")\n",
        " Shows:\n",
        "How many embeddings (chunks) are stored (index.ntotal).\n",
        "A tip: you can preload FAQs in data/faqs.txt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
