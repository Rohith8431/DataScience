{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install streamlit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YD6lJRno16M1",
        "outputId": "81a9ea0f-4236-476a-fc1e-049378d04941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.48.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.48.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m112.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.48.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncYNwM19zEZL"
      },
      "outputs": [],
      "source": [
        "import os   #let's us to interact with computer OS. eg, create/delete folders,read files,check file paths etc...\n",
        "import io   #stands for i/p o/p .Let's us work with data as if it's a file(even if it's in a memory)\n",
        "import re   #Regular expression. helps to find or replace pattern in text\n",
        "import json  #used to work with JSON data(common format to store data like a dict)\n",
        "import pandas as pd  #pandas-->data handling module, popular library for data analysis,works with table(rows and column) like excel\n",
        "import numpy as np  #numpy-->data handling module,library for numerical calculation,great for working with arrays,matrics and math operation\n",
        "import time  #let's us work with time(pause pgm,measure how long something takes)\n",
        "import streamlit as st  #stramlit-->app & dashboard,used to make web apps for DS & ML\n",
        "from typing import List,Dict,Tuple,Any  #type hinting(not code execution,just for clarity),helps to explain datatypes in function"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "e.g for type hinting\n",
        "\n",
        "def add_numbers(numbers: List[int]) -> int:\n",
        "   return sum(numbers)\n",
        "\n",
        "list[int] means the input is a list of integer\n",
        "-> int means the function returns as integer"
      ],
      "metadata": {
        "id": "5mkkaz4D7TMl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import pipeline\n",
        "import faiss\n",
        "from pypdf import PdfReader\n",
        "from docx import Document as DocxDocument"
      ],
      "metadata": {
        "id": "YF5tUjO011hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP & AI models\n",
        "1.from sentence_transformers import SentenceTransformer\n",
        "   library to create sentence embbedding(turns sentence into num vector);useful for sementaic search,recommandation,clustering\n",
        "   Example:\n",
        "   \"I love cats\" → [0.12, -0.55, 0.88, ...] (vector).\n",
        "\n",
        "2.from transformers import pipeline\n",
        "   Hugging Face Transformers library.\n",
        "   pipeline is a shortcut to use pre-trained AI models easily.\n",
        "   Example tasks: text summarization, translation, sentiment analysis, question answering.\n",
        "   Example:\n",
        "   from transformers import pipeline\n",
        "   summarizer = pipeline(\"summarization\")\n",
        "   print(summarizer(\"I love tom\"))\n",
        "\n",
        "Searching Similar Data\n",
        "1.import faiss\n",
        "  FAISS (by Facebook/Meta) is for fast similarity search.\n",
        "  Helps you search quickly through millions of vectors (like sentence embeddings).\n",
        "  Example: find the most similar sentence/document to a query.\n",
        "\n",
        "Reading Files\n",
        "1.from pypdf import PdfReader\n",
        "  Library to read text from PDF files.\n",
        "  Example: load a PDF, get number of pages, extract text.\n",
        "\n",
        "from docx import Document as DocxDocument\n",
        "Library to read and write Word documents (.docx).\n",
        "Example: open a .docx file and read paragraphs, or create a new Word file.\n",
        "\n",
        "\n",
        "In short:\n",
        "\n",
        "SentenceTransformer → turns sentences into numbers (vectors).\n",
        "pipeline → quick way to use AI models (summarize, translate, classify).\n",
        "faiss → finds similar vectors really fast (used for search).\n",
        "PdfReader → extracts text from PDFs.\n",
        "DocxDocument → reads/writes Word docs."
      ],
      "metadata": {
        "id": "Frn8O3wn-XoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8jzc2ap8pyg",
        "outputId": "c999c6f7-91c9-4d3a-93a5-19c00d2c7e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5lrWIhnB8wm0",
        "outputId": "45079d81-7adc-4b41-8b93-305f4be9e14a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-6.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pypdf-6.0.0-py3-none-any.whl (310 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/310.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m307.2/310.5 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-6.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install python-docx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vACXofJm9RaQ",
        "outputId": "28e17567-5c5c-445c-f3bf-58339d0dc8c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.14.1)\n",
            "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/253.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx\n",
            "Successfully installed python-docx-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#clean & chunk text\n",
        "def clean_text(text:str)->str:\n",
        "  text=re.sub(r\"\\s+\",\" \",text)\n",
        "  return text.strip()"
      ],
      "metadata": {
        "id": "lo_99tpw9gcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chunk_text(text:str, chunk_size:int=800, overlap:int=120) -> List[str]:\n",
        "  \"\"\"\n",
        "  Character based chunking(simple & Robust)\n",
        "  Chunk size ~800 chars works well for small model like FLAN-T5\n",
        "  \"\"\"\n",
        "  text = clean_text(text)\n",
        "  chunks = []\n",
        "  start = 0\n",
        "  n = len(text)\n",
        "  while start < n:\n",
        "    end = min(start + chunk_size, n)\n",
        "    chunk = text[start:end]\n",
        "    chunks.append(chunk)\n",
        "    start=end-overlap\n",
        "    if start < 0:\n",
        "      start = 0\n",
        "  return chunks"
      ],
      "metadata": {
        "id": "1CFUGGn6FZ6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#file loader(PDF,DOCX,CSV,TXT)"
      ],
      "metadata": {
        "id": "je2IrqM7HdE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_txt(file_bytes: bytes)->str:\n",
        "  return file_bytes.decode(\"utf-8\",errors=\"ignore\")"
      ],
      "metadata": {
        "id": "67VQ7smJI38p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_pdf(file_bytes: bytes)->str:\n",
        "  with io.BytesIO(file_bytes) as fb:\n",
        "    reader = PdfReader(fb)\n",
        "    texts=[]\n",
        "    for page in reader.pages:\n",
        "      try:\n",
        "        t=page.extract_text() or \"\"\n",
        "      except Exception :\n",
        "        t=\"\"\n",
        "      if t:\n",
        "        texts.append(t)\n",
        "  return \"\\n\".join(texts)"
      ],
      "metadata": {
        "id": "QlEAm7w8JUxY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_docx(file_bytes: bytes)->str:\n",
        "  with io.BytesIO(file_bytes) as fb:\n",
        "    doc=DocxDocument(fb)\n",
        "  return \"/n\".join(p.text for p in doc.paragraphs)"
      ],
      "metadata": {
        "id": "0UPYU4Q0UnkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_csv(file_bytes: bytes)->str:\n",
        "  with io.BytesIO(file_bytes) as fb:\n",
        "    df=pd.read_csv(fb)\n",
        "    #converting to readable FAQ-like table text\n",
        "  return df.to_csv(index=False)"
      ],
      "metadata": {
        "id": "S_dm4tTEZgxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_any(file)->Tuple[str,str]:\n",
        "  name=file.name.lower()\n",
        "  content=file.read()\n",
        "  if name.endswith(\".pdf\"):\n",
        "    return \"pdf\",load_pdf(content)\n",
        "  elif name.endswith(\".docx\"):\n",
        "    return \"docx\",load_docx(content)\n",
        "  elif name.endswith(\".csv\"):\n",
        "    return \"csv\",load_csv(content)\n",
        "  elif name.endswith(\".txt\"):\n",
        "    return \"txt\",load_txt(content)\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported file type.Please upload PDF,DOCX,TXT, OR CSV.\")"
      ],
      "metadata": {
        "id": "bS6DobL1aXrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Embedding + FAISS handling\n",
        "@st.cache_resource\n",
        "def get_embedder():\n",
        "  return SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "pYiSxnPed-VL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In summary, this code defines a function that loads a SentenceTransformer model, and the @st.cache_resource decorator ensures that the model is only loaded once, making your Streamlit app more efficient."
      ],
      "metadata": {
        "id": "IeR-6LvLjC6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_or_load_index(\n",
        "    embedder:SentenceTransformer,\n",
        "    storage_dir:str=\"storage\"\n",
        ") ->Tuple[faiss.IndexFlatL2,List[dict[str,Any]]]:\n",
        "    os.makedirs(storage_dir,exist_ok=True)\n",
        "    index_path=os.path.join(storage_dir,\"faiss.index\")\n",
        "    meta_path=os.path.join(storage_dir,\"meta.npy\")\n",
        "\n",
        "    if os.path.exists(index_path) and os.path.exists(meta_path):\n",
        "        index=faiss.read_index(index_path)\n",
        "        metadata=np.load(meta_path,allow_pickle=True).tolist()\n",
        "    return index,metadata\n",
        "\n",
        "    #Empty new index\n",
        "    index=faiss.IndexFlatL2(384)\n",
        "    metadata: List[Dict[str,Any]]=[]\n",
        "    return index,metadata\n"
      ],
      "metadata": {
        "id": "Krd2u4uvels-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " this function provides a convenient way to persist your FAISS index and its corresponding metadata, allowing you to avoid rebuilding them every time your application runs if the data hasn't changed."
      ],
      "metadata": {
        "id": "tnOKntohjU22"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def persist_index(index: faiss.IndexFlatL2, metadata: List[Dict[str, Any]], storage_dir: str = \"storage\"):\n",
        "    os.makedirs(storage_dir, exist_ok=True)\n",
        "    faiss.write_index(index, os.path.join(storage_dir, \"faiss.index\"))\n",
        "    np.save(os.path.join(storage_dir, \"meta.npy\"), np.array(metadata, dtype=object), allow_pickle=True)"
      ],
      "metadata": {
        "id": "I-vZg7LuhwKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_texts_to_index(\n",
        "    texts: List[str],\n",
        "    source_name: str,\n",
        "    embedder: SentenceTransformer,\n",
        "    index: faiss.IndexFlatL2,\n",
        "    metadata: List[Dict[str,Any]]\n",
        "):\n",
        "    if not texts:\n",
        "      return\n",
        "    vectors = embedder.encode(texts,convert_to_numpy=True,normalize_embeddings=False)\n",
        "    index.add(vectors)\n",
        "    for i,t in enumerate(texts):\n",
        "      metadata.append({\n",
        "          \"source\": source_name,\n",
        "          \"chunk_id\": i,\n",
        "          \"text\": t\n",
        "      })"
      ],
      "metadata": {
        "id": "2hJWt4ENllyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Retriever\n",
        "def retrieve(query: str,embedder,index,metadata,k: int=3):\n",
        "  if index.ntotal == 0:\n",
        "    return[]"
      ],
      "metadata": {
        "id": "2NHT_HatEPK8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}