{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRjzbPkqJQ2U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import RandomOverSampler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv(\"magic04.data\")"
      ],
      "metadata": {
        "id": "Ea6VwZNPJm4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = [\"fLength\",\"fWidth\",\"fSize\",\"fConc\",\"fConc1\",\"fAsym\",\"fM3Long\",\"fM3Trans\",\"fAlpha\",\"fDist\",\"class\"]\n",
        "df = pd.read_csv(\"magic04.data\",names=cols)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "xzAIIuiBKPJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " here from \"flength\" to \"class\" called feature, feature are just thing that we are going to pass our model in order to help us predict the label,which in this case class column.\n",
        "\n",
        " here sample 0 has 10 different feature,so i have 10 different values that i can pass into some model."
      ],
      "metadata": {
        "id": "CW2aEMJWNZze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"class\"].unique()      #g-gamma h-hydra"
      ],
      "metadata": {
        "id": "dhdmdoLbMGTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"class\"]=(df['class']=='g').astype(int)"
      ],
      "metadata": {
        "id": "N7kuEiUdMuKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "-Jcpj1HHzYz3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "hCVV7pLqzap7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for labels in cols[:-1]:\n",
        "  plt.hist(df[df[\"class\"]==1][labels],color='blue',label='gamma',alpha=0.7,density=True)\n",
        "  plt.hist(df[df[\"class\"]==0][labels],color='red',label='hydron',alpha=0.7,density=True)\n",
        "  plt.title(labels)\n",
        "  plt.ylabel(\"Probability\")\n",
        "  plt.xlabel(labels)\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "C8GEdpE2zfz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train Validate Test"
      ],
      "metadata": {
        "id": "6LHMjpog3C71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, validate, test = np.split(df.sample(frac=1), [int(0.6*len(df)), int(0.8*len(df))])      # splitting the dataframe, (df ...) this will shafapal my data, 0.6 means 60% data to validation, 0.8  means 80% data to test"
      ],
      "metadata": {
        "id": "S5NOvBni2OI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "if u seen in df.head() flength has over 100 but in fconc we have less than 100 ; the scale of numbers is way off and somethimes it will effect our results. so we need to scale this relative to mean and deviation of that specific column."
      ],
      "metadata": {
        "id": "k6SkSGY17qZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_dataset(dataframe):\n",
        "  X = dataframe[dataframe.cols[:-1]].values\n",
        "  y = dataframe[dataframe.cols[-1]].values\n",
        "\n",
        "  # import standardScalar\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X = scaler.fit_transform(X)\n",
        "\n",
        "  # now we'll create the lot data as 1 2d array to do that we need hstack\n",
        "  # we are stacking x and y\n",
        "  # numpy is very particular about dimensions\n",
        "  # here x is 2d but y 1d(vector), now to reshape y to 2d, we need call reshape\n",
        "  # and also need to pass dimensions\n",
        "  # here (-1,1) means make this 2d array where -1 means len(y)\n",
        "\n",
        "  data = np.hstack((X,np.reshape(y,(-1,1))))\n",
        "\n",
        "  return data, X, y"
      ],
      "metadata": {
        "id": "wFCKenpg5eOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train[train[\"class\"]==1]))  # gamma\n",
        "print(len(train[train[\"class\"]==0]))"
      ],
      "metadata": {
        "id": "VLbGqbyDAROe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here we have 7k gamma and 4k hydra, this will become a issue, so we need to oversample our training data sets means to increase the number of values, so they match with each other.\n",
        "\n",
        "do reslove this we can import RandomOverSampler"
      ],
      "metadata": {
        "id": "lL_3mLBIB_uW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_dataset(dataframe, oversampler=False):\n",
        "  X = dataframe[dataframe.columns[:-1]].values\n",
        "  y = dataframe[dataframe.columns[-1]].values\n",
        "\n",
        "  scaler = StandardScaler()\n",
        "  X = scaler.fit_transform(X)\n",
        "\n",
        "  if oversampler:\n",
        "    ros = RandomOverSampler()\n",
        "    X,y = ros.fit_resample(X,y)   # says take less class and keep sampling from their to increase the dataset of that smaller class, so that they can match\n",
        "\n",
        "  data = np.hstack((X,np.reshape(y,(-1,1))))\n",
        "\n",
        "  return data, X, y"
      ],
      "metadata": {
        "id": "4cwnTfNSB5_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train, X_train, y_train = scale_dataset(train, oversampler= True)"
      ],
      "metadata": {
        "id": "hHTz4QbaGqTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "40g6QZlJMLxg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}