{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6kE_OVyizEH"
      },
      "outputs": [],
      "source": [
        "pip install PyPDF2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U langchain-community"
      ],
      "metadata": {
        "id": "wD9zUIfNk-VK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS"
      ],
      "metadata": {
        "id": "uiGR7wxRkYlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"\""
      ],
      "metadata": {
        "id": "s7j4sjhdk8T0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdfreader = PdfReader(\"Rohith_CV.pdf\")"
      ],
      "metadata": {
        "id": "iaSYF9LclbTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import Concatenate\n",
        "raw_data=''\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "  content=page.extract_text()\n",
        "  if content:\n",
        "    raw_data+=content"
      ],
      "metadata": {
        "id": "h5tgQD4ymvqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data"
      ],
      "metadata": {
        "id": "vRjV3hq9nkqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_spiltter = CharacterTextSplitter(        # charactertextsplitter used to split the large chunks of text(like pdf content) into smaller overlapping segments,which makes easier to pass to LLM for que-ans or summarize. it splits text based on character and specified separator.\n",
        "    separator='\\n',\n",
        "    chunk_size=800,  #each chunk of text is 800 character long\n",
        "    chunk_overlap=200, #each chunk will overlap the previous one by 200 character\n",
        "    length_function=len,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "u0JOIKYwnl_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=text_spiltter.split_text(raw_data)"
      ],
      "metadata": {
        "id": "iMkCY59ho3K7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "id": "jt6GSnyTpB7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "to count tokens instead of length (i'm doing this on my own)"
      ],
      "metadata": {
        "id": "WwoRwHqpysWS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# Choose tokenizer – for GPT-3.5/GPT-4 use 'cl100k_base'\n",
        "\n",
        "encoding = tiktoken.get_encoding('cl100k_base')"
      ],
      "metadata": {
        "id": "qm6c_ysRpDr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_tokens(text:str)->int:\n",
        "  return len(encoding.encode(text))"
      ],
      "metadata": {
        "id": "TznvOgCJuYGi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "or def count_tokens(raw_data:str)->int:\n",
        "    return len(encoding.encode(raw_data))  \n",
        "this also gives me same result."
      ],
      "metadata": {
        "id": "2UbTnSOo5hqq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_spiltter = CharacterTextSplitter(\n",
        "    separator='/n',\n",
        "    chunk_size=800,\n",
        "    chunk_overlap=200,\n",
        "    length_function=count_tokens\n",
        ")"
      ],
      "metadata": {
        "id": "JnFifK7zu598"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = text_spiltter.split_text(raw_data)"
      ],
      "metadata": {
        "id": "m1PaT8lkvmXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count_tokens(text)"
      ],
      "metadata": {
        "id": "HmA60kDBv2I4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You're passing text — which is a list of strings (the chunks) — to count_tokens(), which expects a single string.\n",
        "\n",
        "ToFix:\n",
        "\n",
        "You need to either:\n",
        "\n",
        "Count tokens for each chunk individually, or\n",
        "\n",
        "Join all chunks back into one string before counting"
      ],
      "metadata": {
        "id": "qUh3y4Fxy5si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count token for each chunck.\n",
        "for i, chunk in enumerate(pdfreader.pages):\n",
        "  print(f\"chunk {i+1} - Tokens: {count_tokens(chunk)}\")"
      ],
      "metadata": {
        "id": "Ydu_E0Sl0DMh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "you're passing a PageObject from PyPDF2 directly to your count_tokens() function — but count_tokens() expects a string, not a page object.\n",
        "\n",
        "tofix\n",
        "\n",
        "we need to extract the text from pages first.\n",
        "\n",
        "\n",
        "why this works.\n",
        "\n",
        "pdfreader.pages[i] returns a PageObject\n",
        "\n",
        "page.extract_text() returns a string\n",
        "\n",
        "count_tokens() expects a string, not a PageObject"
      ],
      "metadata": {
        "id": "XN2bReb76_mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# count token for each page.\n",
        "for i, page in enumerate(pdfreader.pages):\n",
        "  text=page.extract_text()\n",
        "  if text:\n",
        "    print(f\"Page {i+1} - Tokens: {count_tokens(text)}\")\n",
        "  else:\n",
        "    print(f\"Page {i+1} - No extractable text\")"
      ],
      "metadata": {
        "id": "QBxENt8j0qN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Total tokens :\",count_tokens(\" \".join(text)))"
      ],
      "metadata": {
        "id": "HmHgvjCX2qiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# f stands for f-string short for formatted string literal\n",
        "# e.g\n",
        "\n",
        "name = 'rohith'\n",
        "age = 25\n",
        "\n",
        "print(f\"My name is {name} and my age is {age}\")"
      ],
      "metadata": {
        "id": "5VVBuTit-A6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = OpenAIEmbeddings()"
      ],
      "metadata": {
        "id": "J_0m0DBg3ONU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "here api is a kind of password that allows our code to securely access services provided by our external api - in this case OpenAI API\n",
        "\n",
        "OpenAI API key is a unique string (it look like this sk-...) that links API usage to our OpenAI a/c\n",
        "\n",
        "to use in code,\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"sk-...........\"\n",
        "\n",
        "OpenAI API is paid, pay for what i have used-usage is measured in tokens(words or characters).\n",
        "In my openai a/c is there is $5 in free trial means it is for 5$ dollars if not it shows 0.00 then we can't access without paying."
      ],
      "metadata": {
        "id": "4hrIatFwAfpp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_search = FAISS.from_texts(text,embedding)  #text is converting into embedding and stored in doc_search"
      ],
      "metadata": {
        "id": "szIgNHHX-wxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_search"
      ],
      "metadata": {
        "id": "_UwXzLB1UEH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "laQa0RbNUlAj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = load_qa_chain(OpenAI(),chain_type='stuff')"
      ],
      "metadata": {
        "id": "KpdH2LFmVV5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \" \"\n",
        "docs = doc_search.similarity_search(query)\n",
        "chain.run(input_documents=docs, questions=query)"
      ],
      "metadata": {
        "id": "G-EjNS5kVn-c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}